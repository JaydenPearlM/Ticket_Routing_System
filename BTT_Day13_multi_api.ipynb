{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0f7ca-68e6-43fc-b910-13c195e6e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAIVE BAYES pipeline + TF-IDF\n",
    "\n",
    "#Ticket Routeing with Fastapi\n",
    "\n",
    "# 'You're File Here' is where you put your Data.csv\n",
    "# 'Column' is where you want to use as a Column such as:  \n",
    "# 'ID, groups etc\n",
    "# Data not Included\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c7d4c-e0e4-4f24-b700-5f8e0169686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import threading\n",
    "import pickle\n",
    "import uvicorn\n",
    "import uuid\n",
    "import json\n",
    "import nest_asyncio\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi import UploadFile, File, BackgroundTasks\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill, Font\n",
    "from openpyxl.utils import get_column_letter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f35156-7a58-4baf-aa9a-b2854ad98a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#File path for the data to be cleaned\n",
    "file_path = r\"Youre File Here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ce51d-aef7-4595-8d64-68e6f7d6eda7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load Data\n",
    "df_raw = pd.read_csv(file_path)\n",
    "df_raw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f784c7d6-0129-4aec-9cd8-8aa63992241f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Cleaning the Data - duplicates, Special Characters, normalizeation, check for NaNs, whitespace, Outliers\n",
    "\n",
    "#confirm before\n",
    "print(\"Missing values before cleaning: \")\n",
    "print(df_raw.isna().sum())\n",
    "print(\"Duplicate rows before cleaning:\", df_raw.duplicated().sum())\n",
    "print(\"Original rows:\", len(df_raw))\n",
    "print(\"Columns before removing duplicates:\", len(df_raw.columns))\n",
    "\n",
    "#remove rows with NaNs\n",
    "df_cleaned = df_raw.dropna()\n",
    "#remove duplicates\n",
    "df_cleaned = df_raw.drop_duplicates()\n",
    "#remove special characters\n",
    "df_cleaned = df_raw.replace(r'[^\\w\\s.,-]', '', regex=True)\n",
    "\n",
    "#confirm after\n",
    "print(\"\\nMissing values after cleaning: \")\n",
    "print(df_cleaned.isna().sum())\n",
    "print(\"Cleaned rows:\", len(df_cleaned))\n",
    "print(\"Duplicate rows after cleaning:\", df_cleaned.duplicated().sum()) \n",
    "print(\"Columns after removing duplicates:\", len(df_cleaned.columns))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c12b4f-fc19-495a-99ab-6342e2de461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to CSV for savings\n",
    "output_path = \"Youre File Here\"\n",
    "df_cleaned.to_csv(output_path, index=False)\n",
    "print(f\"Cleaned data successfully saved to '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844e6ac8-ed93-4c30-8deb-3dbcac2c58e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull what is needed for the data - short_description type str, assignment_group type str priority type str\n",
    "df = pd.read_csv(r\"Youre File Here\")\n",
    "df_cleaned = df[['short_description', 'assignment_group', 'priority']]\n",
    "df['short_description']\n",
    "df['assignment_group']\n",
    "df['priority']\n",
    "df_cleaned.head(10)\n",
    "\n",
    "output_path = \"Youre File Here\"\n",
    "df_cleaned.to_csv(output_path, index=False)\n",
    "print(f\"Cleaned data successfully saved to '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a035bc1-2436-422e-8b7b-a5539c66a053",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#imbalanced Data Check\n",
    "print(\"This is the short description column: \")\n",
    "print(df['Your Column Here'].value_counts(normalize=True)) #Balanced\n",
    "\n",
    "print(\"\\nThis is the assignment_group column: \")   \n",
    "print(df['Your Column Here'].value_counts(normalize=True)) #unBalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39936a3-2a67-4bef-b7f6-0ea52b86ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the to be Balanced Data\n",
    "df_before = pd.read_csv(r'C:\\Users\\2130537\\Desktop\\Giant_eagle\\data_clean_balanced\\Before_balance.csv')\n",
    "\n",
    "X_raw = df[['Column', 'column', 'column']].fillna(' ')\n",
    "\n",
    "# Step 1: Extract X and y from the cleaned DataFrame\n",
    "X_raw = df_before['Column'].astype(str) + '  ' + df_before[\"Column'].astype(str)\n",
    "y_raw = df_before['Column'].astype(str)\n",
    "\n",
    "# Step 2: Encode assignment groups\n",
    "le_target = LabelEncoder()\n",
    "y_labels = le_target.fit_transform(y_raw)\n",
    "\n",
    "#save\n",
    "joblib.dump(le_target, r\"Youre File Here\")\n",
    "\n",
    "# Step 3: Filter out labels with too few samples\n",
    "label_counts = Counter(y_labels)\n",
    "#print(\"Original class distribution:\", label_counts)\n",
    "\n",
    "#step 4\n",
    "min_samples = 4\n",
    "valid_classes = [label for label, count in label_counts.items() if count >= min_samples]\n",
    "mask = np.isin(y_labels, valid_classes)\n",
    "\n",
    "# Step 5\n",
    "X_filtered = X_raw[mask]\n",
    "y_filtered = y_labels[mask]\n",
    "y_raw_filtered = y_raw[mask]\n",
    "\n",
    "# Step 6 Splitting Data before SMOTE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered)\n",
    "\n",
    "# Step 7: TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Step 8: Plot class distribution\n",
    "plt.figure(figsize=(9, 7))\n",
    "y_raw_filtered = y_raw[mask]  # Only show filtered classes\n",
    "y_raw_filtered.value_counts().plot(kind='bar', color='red')\n",
    "plt.xticks(rotation=90)\n",
    "plt.locator_params(axis='x', nbins=50)\n",
    "plt.title('Class Distribution Before SMOTE')\n",
    "plt.xlabel('Youre File Here')\n",
    "plt.ylabel('Number of Tickets')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62999f5b-2554-4495-9c2f-c49297696507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', 10)\n",
    "print(df_before['Youre File Here'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e118a800-453a-4899-8cd8-22462f5e2e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying SMOTE to balance the classes\n",
    "#X_filtered is a dataframe and y_filtered is a series\n",
    "\n",
    "# 1. Apply SMOTE\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vec, y_train)\n",
    "\n",
    "# 2. Decode y_resampled back to original labels\n",
    "y_decoded = le_target.inverse_transform(y_train_resampled)\n",
    "\n",
    "# 4. Build DataFrame\n",
    "df_resampled = pd.DataFrame(X_train_resampled)\n",
    "df_resampled['Your File Here'] = y_decoded\n",
    "\n",
    "# 5. Plot class distribution\n",
    "resampled_counts = Counter(y_decoded)\n",
    "top_10 = resampled_counts.most_common(10)\n",
    "\n",
    "labels, counts = zip(*top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09cb384-6e06-4704-b42b-fb11cf712585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Save to 0CSV\n",
    "df_resampled.to_csv(r\"Youre File Here\")\n",
    "print(\"Successfully Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d439dcf-5bb2-44ea-be24-108c26075d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = pd.read_csv(r\"Youre File Here\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(labels, counts, color='purple')\n",
    "plt.xlabel(\"\"Youre File Here\"\")\n",
    "plt.ylabel(\"Number of Tickets\")\n",
    "plt.title(\"Class Distrabution After SMOTE\")\n",
    "plt.xticks(rotation=90, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3d3ed-03f1-4029-8e0a-bdc6bf44b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive TF-IDF     \n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_resampled, y_train_resampled)  #Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b86e7b-c40e-4a45-bea1-43275c7aae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "y_pred = nb_model.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b8f6b-8371-4a13-a517-8f383fe78cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# === Setup ===\n",
    "model_name = \"Department Parser full\"\n",
    "vectorizer_name = \"Vectorizer\"\n",
    "\n",
    "model_folder = r\"Youre File Here\"\n",
    "vectorizer_folder = r\"Youre File Here\"\n",
    "json_only_folder = r\"Youre File Here\"\n",
    "\n",
    "# Create folders if they don't exist\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "os.makedirs(vectorizer_folder, exist_ok=True)\n",
    "os.makedirs(json_only_folder, exist_ok=True)\n",
    "\n",
    "# === Timestamp for file versions ===\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# === Save the Model ===\n",
    "model_filename = f\"{model_name}_{timestamp}.pkl\"\n",
    "model_path = os.path.join(model_folder, model_filename)\n",
    "joblib.dump(nb_model, model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# === Save the Vectorizer ===\n",
    "vectorizer_filename = f\"{vectorizer_name}_{timestamp}.pkl\"\n",
    "vectorizer_path = os.path.join(vectorizer_folder, vectorizer_filename)\n",
    "joblib.dump(vectorizer, vectorizer_path)\n",
    "print(f\"Vectorizer saved to: {vectorizer_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea7830-69b3-44e6-beb7-41a4a6c86ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import os\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "\n",
    "nest_asyncio.apply()  # For notebook use\n",
    "\n",
    "# === Folder paths ===\n",
    "MODEL_FOLDER = Path(r\"Youre File Here\")\n",
    "VECTORIZER_FOLDER = Path(r\"Youre File Here\"\n",
    "\n",
    "MODEL_FOLDER.mkdir(exist_ok=True)\n",
    "VECTORIZER_FOLDER.mkdir(exist_ok=True)\n",
    "\n",
    "# === Load most recent .pkl file ===\n",
    "def get_latest_file(folder: Path):\n",
    "    files = list(folder.glob(\"*.pkl\"))\n",
    "    return max(files, key=lambda f: f.stat().st_mtime) if files else None\n",
    "\n",
    "# === Load model ===\n",
    "model_path = get_latest_file(MODEL_FOLDER)\n",
    "if model_path is None:\n",
    "    raise FileNotFoundError(\"No model file found!\")\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# === Load vectorizer ===\n",
    "vectorizer_path = get_latest_file(VECTORIZER_FOLDER)\n",
    "if vectorizer_path is None:\n",
    "    raise FileNotFoundError(\" No vectorizer file found!\")\n",
    "vectorizer = joblib.load(vectorizer_path)\n",
    "\n",
    "# === FastAPI ===\n",
    "app = FastAPI(title=\"Ticket Department Classifier API\")\n",
    "\n",
    "label_encoder = joblib.load(r\"Youre File Here\")\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "class PredictRequest(BaseModel):\n",
    "    short_description: Union[str, None] = None\n",
    "    short_descriptions: Union[List[str], None] = None\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"message\": \"Model API is up!\"}\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(request: PredictRequest) -> Dict[str, Union[str, List[str]]]:\n",
    "    try:\n",
    "        # Handle single or multiple inputs\n",
    "        if request.short_description:\n",
    "            descriptions = [request.short_description]\n",
    "        elif request.short_descriptions:\n",
    "            descriptions = request.short_descriptions\n",
    "        else:\n",
    "            raise ValueError(\"You must provide either 'Column' or 'Column')\n",
    "\n",
    "        # Vectorize and predict\n",
    "        transformed = vectorizer.transform(descriptions)\n",
    "        predictions = model.predict(transformed)\n",
    "        departments = label_encoder.inverse_transform(predictions).tolist()\n",
    "\n",
    "        # Return single or multiple results\n",
    "        if len(departments) == 1:\n",
    "            return {\"Column\": departments[0]}\n",
    "        else:\n",
    "            return {\"Column\": departments}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Internal server error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Only run with: python script.py\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
